vegetation_data <-xmlToDataFrame(doc)
file <- "http://www.ospo.noaa.gov/data/land/bbep2/biomass_burning.txt? filename=LocalFile.txt.gz&dir=C:/R/Data"
Biomass_Burning_Data <- read.csv(file, header=TRUE)
url <- "http://www.globalfirepower.com/oil-production-by-country.asp"
oil_production_data = readHTMLTable(url, which=2)
library("tibble")
library(xml2)
# General-purpose data wrangling
library(tidyverse)
# Parsing of HTML/XML files
library(rvest)
# String manipulation
library(stringr)
# Verbose regular expressions
library(rebus)
# Eases DateTime manipulation
library(lubridate)
library(rvest)
library(tidyverse)
library(XML)
library(httr)
install.packages("tidyverse")
file <- "http://www.ospo.noaa.gov/data/land/bbep2/biomass_burning.txt? filename=LocalFile.txt.gz&dir=C:/R/Data"
Biomass_Burning_Data <- read.csv(file, header=TRUE)
url <- "http://www.globalfirepower.com/oil-production-by-country.asp"
oil_production_data = readHTMLTable(url, which=2)
url <- "https://www.globalfirepower.com/oil-production-by-country.asp"
oil_production_data = readHTMLTable(url, which=2)
oil_production_data = readHTMLTable(url, which=2)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
doc <- htmlParse(rawToChar(GET(url)$content))
vegetation_data <-xmlToDataFrame(doc)
install.packages("XML2")
install.packages("XMl2")
install.packages("XM12")
install.packages("XM:2")
install.packages("XML2")
install.packages("xml2")
install.packages("xml2")
library(selectr)
library(xml2)
library(xm12)
library(xml2)
install.packages("xml2")
library(xml2)
url <- 'http://pgdbablog.wordpress.com/2015/12/10/pre-semester-at-iim-calcutta/'
webpage <- read_html(url)
vignette("selectorgadget")
javascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})();
rating_html=html_nodes(webpage,'.imdb-rating')   #’.imdb-rating’ is taken from CSS selector
rating_html=html_nodes(webpage,'.imdb-rating')   #’.imdb-rating’ is taken from CSS selector
library("tibble")
library(xml2)
# General-purpose data wrangling
library(tidyverse)
# Parsing of HTML/XML files
library(rvest)
# String manipulation
library(stringr)
# Verbose regular expressions
library(rebus)
# Eases DateTime manipulation
library(lubridate)
library(rvest)
library(tidyverse)
library(XML)
library(httr)
library(selectr)
library(xml2)
rating_html=html_nodes(webpage,'.imdb-rating')   #’.imdb-rating’ is taken from CSS selector
#Converting the rating data to text
rating <- html_text(rating_html)
#Check the rating captured
rating
#Check the rating captured
print(rating)
post_date_html <- html_nodes(webpage,'.entry-date')
#Converting the post date to text
post_date <- html_text(post_date_html)
#Verify the date captured
post_date
title_summary_html <- html_nodes(webpage,'em')
#Converting the title data to text
title_summary <- html_text(title_summary_html)
#Check the title of the article
title_summary[2]
#Read the title summary of the article
title_summary[1]
#Using CSS selectors to scrap the blog content
content_data_html <- html_nodes(webpage,'p')
#Converting the blog content data to text
content_data <- html_text(content_data_html)
#Let's see how much content we have captured
length(content_data) #the output is 38
content_data[1]
content_data[2]
content_data[3]
content_data[4]
content_data[5]
content_data[6]
content_data[7]
content_data[8]
content_data[9]
content_data[10]
#Using CSS selectors to scrap the names of people who commented
comments_html <- html_nodes(webpage,'.fn')
#Converting the commenters to text
comments <- html_text(comments_html)
#Let's have a look at all the names
comments
#What are the total number of comments made?
length(comments) #8 comments
#How many different people made comments?
length(unique(comments)) #6 people
#convert all the data into a data frame
first_blog<-data.frame(Date = post_date, Title = title_summary[2],Description = title_summary[1], content=paste(content_data[1:11], collapse = ''), commenters=length(comments))
#Checking the structure of the data frame
str(first_blog) #all the features are factors and need to be converted into appropriate types
#Checking the structure of the data frame
str(first_blog) #all the features are factors and need to be converted into appropriate types
#Specifying the url for desired website to be scrapped
url <- 'http://pgdbablog.wordpress.com/2015/12/18/pgdba-chronicles-first-semester/'
#Reading the HTML code from the website
webpage <- read_html(url)
#Using CSS selectors to scrap the rankings section
post_date_html <- html_nodes(webpage,'.entry-date')
#Converting the ranking data to text
post_date <- html_text(post_date_html)
#Let's have a look at the rankings
post_date
#Using CSS selectors to scrap the title section
title_summary_html <- html_nodes(webpage,'em')
#Converting the title data to text
title_summary <- html_text(title_summary_html)
#Let's have a look at the title
title_summary[1]
title_summary[2]
title_summary[3]
title_summary[4]
title_summary[5]
title_summary[6]
title_summary[6]any different people made comments?
length(unique(comments)) #6 people
title_summary[6]any different people made comments?
length(unique(comments)) #6 people
title_summary[6]
title_summary[5]
title_summary[6]
length(unique(comments)) #6 people
#convert all the data into a data frame
first_blog<-data.frame(Date = post_date, Title = title_summary[2],Description = title_summary[1], content=paste(content_data[1:11], collapse = ''), commenters=length(comments))
#Checking the structure of the data frame
str(first_blog) #all the features are factors and need to be converted into appropriate types
#Setting an html_session
webpage <- html_session(url)
#Getting the image using the tag
Image_link <- webpage %>% html_nodes(".wp-image-54")
#Fetch the url to the image
img.url <- Image_link[1] %>% html_attr("src")
#Save the image as a jpeg file in the working directory
download.file(img.url, "test.jpg", mode = "wb")
library(dplyr)   #Data manipulation
library(forcats) #ggplot frequency
library(ggplot2) #visualizations
library(caTools) #Data wrangling
library("tm")  # for text mining
library("SnowballC") # for text stemming
library("wordcloud") # word-cloud generator
library("RColorBrewer") # color palettes
library(randomForest) #randomforest
library(dplyr)   #Data manipulation
library(dplyr)   #Data manipulation
#Save the image as a jpeg file in the working directory
download.file(img.url, "test.jpg", mode = "wb")
library(DT)
library(tidytext)
library(dplyr)
library(stringr)
library(sentimentr)
library(ggplot2)
library(RColorBrewer)
library(readr)
library(SnowballC)
library(tm)
library(wordcloud)
library(reticulate)
library(crfsuite)
Hotel_data <- fread("KEEP.csv")
install.packages(c("crfsuite", "DT", "reticulate"))
library(DT)
library(tidytext)
library(dplyr)
library(stringr)
library(sentimentr)
library(ggplot2)
library(RColorBrewer)
library(readr)
library(SnowballC)
library(tm)
library(wordcloud)
library(reticulate)
library(crfsuite)
Hotel_data <- fread("KEEP.csv")
library(ggplot2) #Visualizations (also included in the tidyverse package)
library(ggrepel) #`geom_label_repel`
library(gridExtra) #`grid.arrange()` for multi-graphs
library(knitr) #Create nicely formatted output tables
library(kableExtra) #Create nicely formatted output tables
library(formattable) #For the color_tile function
library(circlize) #Visualizations - chord diagram
library(memery) #Memes - images with plots
library(magick) #Memes - images with plots (image_read)
library(yarrr)  #Pirate plot
library(radarchart) #Visualizations
library(igraph) #ngram network diagrams
library(ggraph) #ngram network diagrams
library(data.table)
Hotel_data <- fread("KEEP.csv")
install.packages("esquisse")
esquisse::esquisser()
library(esquisse)
esquisse::esquisser()
library(esquisse)
esquisse::esquisser()
library(ggplot2)
library(esquisse)
library(ggplot2)
library(ggplot)
library(esquisse)
library(ggplot2)
library(esquisse)
esquisse::esquisser()
library(ggplot2)
library(esquisse)
esquisse::esquisser()
library(readxl)
REALIS2019 <- read_excel("C:/Users/tecky/Desktop/REALIS2019.csv")
View(REALIS2019)
library(ggplot2)
library(esquisse)
REALIS2019 <- read_excel("C:/Users/tecky/Desktop/REALIS2019.csv")
esquisse::esquisser()
REALIS2019 <- read_excel("C:/Users/tecky/Desktop/REALIS2019.csv")
REALIS2019 <- read_csv("C:/Users/tecky/Desktop/REALIS2019.csv")
REALIS2019 <- read.csv("C:/Users/tecky/Desktop/REALIS2019.csv")
View(REALIS2019)
library(ggplot2)
library(esquisse)
esquisse::esquisser()
ggplot(REALIS2019) +
aes(x = Property.Type, y = Unit.Price....psf.) +
geom_boxplot(fill = "#0c4c8a") +
theme_minimal() +
facet_wrap(vars(Planning.Region))
packages = c('treemap','tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2', 'devtools')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
library("geofacet")
get_grid_names()
head(state_ranks)
grids <- sg_planning_area_grid1.csv
grids <- read_csv(sg_planning_area_grid1.csv)
grids <- read_csv("sg_planning_area_grid1.csv")
get_grid_names()
get_grid_names()
head(sg_planning_area_grid1)
head(state_ranks)
get_grid_names()
ggplot(state_unemp, aes(year, rate)) +
geom_line() +
facet_geo(~ state, grid = "us_state_grid2", label = "name") +
scale_x_continuous(labels = function(x) paste0("'", substr(x, 3, 4))) +
labs(title = "Seasonally Adjusted US Unemployment Rate 2000-2016",
caption = "Data Source: bls.gov",
x = "Year",
y = "Unemployment Rate (%)") +
theme(strip.text.x = element_text(size = 6))
head(state_unemp)
ggplot(state_unemp, aes(year, rate)) +
geom_line() +
facet_geo(~ state, grid = "us_state_grid2", label = "name") +
scale_x_continuous(labels = function(x) paste0("'", substr(x, 3, 4))) +
labs(title = "Seasonally Adjusted US Unemployment Rate 2000-2016",
caption = "Data Source: bls.gov",
x = "Year",
y = "Unemployment Rate (%)") +
theme(strip.text.x = element_text(size = 6))
head(us_state_grid2)
head(state_unemp)
runApp('C:/Users/tecky/Desktop/Y3S2/Visual Analytics/Project/WeHouse')
runApp('C:/Users/tecky/Desktop/Y3S2/Visual Analytics/Project/WeHouse')
setwd("C:/Users/tecky/Desktop/Y3S2/Visual Analytics/Project/WeHouse")
runApp('C:/Users/tecky/Desktop/Y3S2/Visual Analytics/Project/Git')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install_github("shiny", "rstudio")
install_github("shiny", "rstudio")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install_github('hadley/ggplot2')
library(ggplotly)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
Overview <- read_csv("data/Overview1.csv")
View(Overview)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
Overview_scatter <- read_csv("data/Overview2.csv")
head(Overview_scatter)
head(select_data)
select_data <- read_csv('data/Map.csv')
head(select_data)
View(select_data)
View(Overview)
runApp()
runApp()
runApp()
runApp()
runApp()
head(select_data)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
help(ggplotly)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages("pxR")
realis <- read_csv("data/TreeMap.csv")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
Overview_scatter <- read_csv("data/Overview2.csv")
head(Overview_scatter)
view(Overview_scatter)
Scatter <- Overview_scatter %>%
group_by(year, resale_price, `HDB Town`) %>%
filter(year==input$year, `HDB Town`==input$HDB)
summarize(`Remaining Lease Years`= mean(remaining_lease), `Unit Area (SQM)` = mean(floor_area_sqm))
head(scatter)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
