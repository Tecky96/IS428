# Save it to a tibble
top_artists <- tibble('Artist' = gsub("\n", "", artist),'Rank' = rank) %>% filter(rank <= 10)
genius_urls <- paste0("https://genius.com/artists/",top_artists$Artist)
h <- read_html("https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives …")
h <- read_html("https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives")
reps <- h %>%
html_node("#mw-content-text > div > table:nth-child(18)") %>%
html_table()
reps <- h %>%
html_node("#mw-content-text > div > table:nth-child(18)") %>%
html_table()
reps <- reps[,c(1:2,4:9)] %>%
as_tibble()
reps <- h %>%
html_node("#mw-content-text > div > table:nth-child(18)") %>%
html_table()
reps <- reps[,c(1:2,4:9)] %>% as_tibble()
h <- read_html("https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives")
reps <- h %>%
html_node("#mw-content-text > div > table:nth-child(18)") %>%
html_table()
reps <- reps[,c(1:2,4:9)] %>% as_tibble()
reps <- h %>%
html_node(xpath = '//*[@id="votingmembers"]') %>%
html_table()
reps <- h %>%
html_node(xpath = '//*[@id="votingmembers"]') %>%
html_table()
reps <- h %>%
fill=TRUE
reps <- h %>%
fill=TRUE
h <- read_html("https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives",fill=TRUE)
h <- read_html("https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives")
reps <- h %>%
html_node(xpath = '//*[@id="votingmembers"]') %>%
html_table(fill=TRUE) %>%
html_table()
h <- read_html("wiki <- read_html("https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives")
")
h <- read_html("wiki <- read_html('https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives')")
h <- read_html("wiki <- read_html('https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives')")
reps <- h %>%
html_node(xpath = '//*[@id="votingmembers"]') %>%
html_table(fill=TRUE) %>%
html_table()
file <- "http://www.ospo.noaa.gov/data/land/bbep2/biomass_burning.txt? filename=LocalFile.txt.gz&dir=C:/R/Data"
Biomass_Burning_Data <- read.csv(file, header=TRUE)
library(XML)
install.packages("XML")
library(XML)
install.packages("rvest")
# Parsing of HTML/XML files
library(rvest)
library(XML)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
vegetation_data <-xmlToDataFrame(url)
library(rjson)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
vegetation_data <-xmlToDataFrame(url)
library(rjson)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344?outFormat=json"
raw_data_json <- scan(url, “”, sep=”\n”)
vegetation_data <- fromJSON(raw_data_json)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
vegetation_data <-xmlToDataFrame(url)
vegetation_data <-xmlToDataFrame(url)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
vegetation_data <-xmlToDataFrame(url)
library(httr)
doc <- htmlParse(rawToChar(GET(url)$content))
vegetation_data <-xmlToDataFrame(doc)
vegetation_data <-xmlToDataFrame(doc)
library(rjson)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344?outFormat=json"
raw_data_json <- scan(url, “”, sep=”\n”)
doc <- htmlParse(rawToChar(GET(url)$content))
vegetation_data <-xmlToDataFrame(doc)
file <- "http://www.ospo.noaa.gov/data/land/bbep2/biomass_burning.txt? filename=LocalFile.txt.gz&dir=C:/R/Data"
Biomass_Burning_Data <- read.csv(file, header=TRUE)
url <- "http://www.globalfirepower.com/oil-production-by-country.asp"
oil_production_data = readHTMLTable(url, which=2)
library("tibble")
library(xml2)
# General-purpose data wrangling
library(tidyverse)
# Parsing of HTML/XML files
library(rvest)
# String manipulation
library(stringr)
# Verbose regular expressions
library(rebus)
# Eases DateTime manipulation
library(lubridate)
library(rvest)
library(tidyverse)
library(XML)
library(httr)
install.packages("tidyverse")
file <- "http://www.ospo.noaa.gov/data/land/bbep2/biomass_burning.txt? filename=LocalFile.txt.gz&dir=C:/R/Data"
Biomass_Burning_Data <- read.csv(file, header=TRUE)
url <- "http://www.globalfirepower.com/oil-production-by-country.asp"
oil_production_data = readHTMLTable(url, which=2)
url <- "https://www.globalfirepower.com/oil-production-by-country.asp"
oil_production_data = readHTMLTable(url, which=2)
oil_production_data = readHTMLTable(url, which=2)
url <- "http://opendata.arcgis.com/datasets/aade6a582a1641078cda28eab3fda344"
doc <- htmlParse(rawToChar(GET(url)$content))
vegetation_data <-xmlToDataFrame(doc)
install.packages("XML2")
install.packages("XMl2")
install.packages("XM12")
install.packages("XM:2")
install.packages("XML2")
install.packages("xml2")
install.packages("xml2")
library(selectr)
library(xml2)
library(xm12)
library(xml2)
install.packages("xml2")
library(xml2)
url <- 'http://pgdbablog.wordpress.com/2015/12/10/pre-semester-at-iim-calcutta/'
webpage <- read_html(url)
vignette("selectorgadget")
javascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})();
rating_html=html_nodes(webpage,'.imdb-rating')   #’.imdb-rating’ is taken from CSS selector
rating_html=html_nodes(webpage,'.imdb-rating')   #’.imdb-rating’ is taken from CSS selector
library("tibble")
library(xml2)
# General-purpose data wrangling
library(tidyverse)
# Parsing of HTML/XML files
library(rvest)
# String manipulation
library(stringr)
# Verbose regular expressions
library(rebus)
# Eases DateTime manipulation
library(lubridate)
library(rvest)
library(tidyverse)
library(XML)
library(httr)
library(selectr)
library(xml2)
rating_html=html_nodes(webpage,'.imdb-rating')   #’.imdb-rating’ is taken from CSS selector
#Converting the rating data to text
rating <- html_text(rating_html)
#Check the rating captured
rating
#Check the rating captured
print(rating)
post_date_html <- html_nodes(webpage,'.entry-date')
#Converting the post date to text
post_date <- html_text(post_date_html)
#Verify the date captured
post_date
title_summary_html <- html_nodes(webpage,'em')
#Converting the title data to text
title_summary <- html_text(title_summary_html)
#Check the title of the article
title_summary[2]
#Read the title summary of the article
title_summary[1]
#Using CSS selectors to scrap the blog content
content_data_html <- html_nodes(webpage,'p')
#Converting the blog content data to text
content_data <- html_text(content_data_html)
#Let's see how much content we have captured
length(content_data) #the output is 38
content_data[1]
content_data[2]
content_data[3]
content_data[4]
content_data[5]
content_data[6]
content_data[7]
content_data[8]
content_data[9]
content_data[10]
#Using CSS selectors to scrap the names of people who commented
comments_html <- html_nodes(webpage,'.fn')
#Converting the commenters to text
comments <- html_text(comments_html)
#Let's have a look at all the names
comments
#What are the total number of comments made?
length(comments) #8 comments
#How many different people made comments?
length(unique(comments)) #6 people
#convert all the data into a data frame
first_blog<-data.frame(Date = post_date, Title = title_summary[2],Description = title_summary[1], content=paste(content_data[1:11], collapse = ''), commenters=length(comments))
#Checking the structure of the data frame
str(first_blog) #all the features are factors and need to be converted into appropriate types
#Checking the structure of the data frame
str(first_blog) #all the features are factors and need to be converted into appropriate types
#Specifying the url for desired website to be scrapped
url <- 'http://pgdbablog.wordpress.com/2015/12/18/pgdba-chronicles-first-semester/'
#Reading the HTML code from the website
webpage <- read_html(url)
#Using CSS selectors to scrap the rankings section
post_date_html <- html_nodes(webpage,'.entry-date')
#Converting the ranking data to text
post_date <- html_text(post_date_html)
#Let's have a look at the rankings
post_date
#Using CSS selectors to scrap the title section
title_summary_html <- html_nodes(webpage,'em')
#Converting the title data to text
title_summary <- html_text(title_summary_html)
#Let's have a look at the title
title_summary[1]
title_summary[2]
title_summary[3]
title_summary[4]
title_summary[5]
title_summary[6]
title_summary[6]any different people made comments?
length(unique(comments)) #6 people
title_summary[6]any different people made comments?
length(unique(comments)) #6 people
title_summary[6]
title_summary[5]
title_summary[6]
length(unique(comments)) #6 people
#convert all the data into a data frame
first_blog<-data.frame(Date = post_date, Title = title_summary[2],Description = title_summary[1], content=paste(content_data[1:11], collapse = ''), commenters=length(comments))
#Checking the structure of the data frame
str(first_blog) #all the features are factors and need to be converted into appropriate types
#Setting an html_session
webpage <- html_session(url)
#Getting the image using the tag
Image_link <- webpage %>% html_nodes(".wp-image-54")
#Fetch the url to the image
img.url <- Image_link[1] %>% html_attr("src")
#Save the image as a jpeg file in the working directory
download.file(img.url, "test.jpg", mode = "wb")
library(dplyr)   #Data manipulation
library(forcats) #ggplot frequency
library(ggplot2) #visualizations
library(caTools) #Data wrangling
library("tm")  # for text mining
library("SnowballC") # for text stemming
library("wordcloud") # word-cloud generator
library("RColorBrewer") # color palettes
library(randomForest) #randomforest
library(dplyr)   #Data manipulation
library(dplyr)   #Data manipulation
#Save the image as a jpeg file in the working directory
download.file(img.url, "test.jpg", mode = "wb")
library(DT)
library(tidytext)
library(dplyr)
library(stringr)
library(sentimentr)
library(ggplot2)
library(RColorBrewer)
library(readr)
library(SnowballC)
library(tm)
library(wordcloud)
library(reticulate)
library(crfsuite)
Hotel_data <- fread("KEEP.csv")
install.packages(c("crfsuite", "DT", "reticulate"))
library(DT)
library(tidytext)
library(dplyr)
library(stringr)
library(sentimentr)
library(ggplot2)
library(RColorBrewer)
library(readr)
library(SnowballC)
library(tm)
library(wordcloud)
library(reticulate)
library(crfsuite)
Hotel_data <- fread("KEEP.csv")
library(ggplot2) #Visualizations (also included in the tidyverse package)
library(ggrepel) #`geom_label_repel`
library(gridExtra) #`grid.arrange()` for multi-graphs
library(knitr) #Create nicely formatted output tables
library(kableExtra) #Create nicely formatted output tables
library(formattable) #For the color_tile function
library(circlize) #Visualizations - chord diagram
library(memery) #Memes - images with plots
library(magick) #Memes - images with plots (image_read)
library(yarrr)  #Pirate plot
library(radarchart) #Visualizations
library(igraph) #ngram network diagrams
library(ggraph) #ngram network diagrams
library(data.table)
Hotel_data <- fread("KEEP.csv")
setwd("C:/Users/tecky/Desktop/Y3S2/Visual Analytics/Project/WeHouse")
install.packages("shiny")
shiny::runApp()
runApp()
install.packages("DT")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp('C:/Users/tecky/Desktop/Y3S2/Visual Analytics/Week 11/In-Class Ex 10/test.R')
runApp('testing.R')
runApp('test.R')
runApp('test.R')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(realis_treemap)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V4.csv")
realis_grouped <- group_by(realis,
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_summarised <- summarise(realis_grouped,
`Total Unit Sold` = sum(Sales, na.rm = TRUE),
`Total Area` = sum(`Area (SQM)`, na.rm = TRUE),
`Unit Price` =  median(`Resale Price`,na.rm=TRUE),
`Average Cost of Area (PSF)` =  mean(`Area Unit (PSF)`,na.rm=TRUE))
View(realis_summarised)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V4.csv")
realis_grouped <- group_by(realis,
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_summarised <- summarise(realis_grouped,
`Total Unit Sold` = sum(Sales, na.rm = TRUE),
`Total Area` = sum(`Area (SQM)`, na.rm = TRUE),
`Average Resale Price` =  median(`Resale Price`,na.rm=TRUE),
`Unit Price (PSF)` =  mean(`Area Unit (PSF)`,na.rm=TRUE))
View(realis_summarised)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(realis_grouped)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_summarised <- summarise(realis_grouped,
`Total Unit Sold` = sum(Sales, na.rm = TRUE),
`Total Area` = sum(`Area (SQM)`, na.rm = TRUE),
`Average Resale Price` =  median(`Resale Price`,na.rm=TRUE),
`Unit Price (PSF)` =  mean(`Area Unit (PSF)`,na.rm=TRUE))
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_summarised <- summarise(realis_grouped,
`Total Unit Sold` = sum(Sales, na.rm = TRUE),
`Total Area` = sum(`Area (SQM)`, na.rm = TRUE),
`Average Resale Price` =  median(`Resale Price`,na.rm=TRUE),
`Unit Price (PSF)` =  mean(`Area Unit (PSF)`,na.rm=TRUE))
View(realis_summarised)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_summarised <- summarise(realis_grouped,
`Total Unit Sold` = sum(Sales, na.rm = TRUE),
`Total Area` = sum(`Area (SQM)`, na.rm = TRUE),
`Average Resale Price` =  median(`Resale Price`,na.rm=TRUE),
`Unit Price (PSF)` =  mean(`Area Unit (PSF)`,na.rm=TRUE))
View(realis)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Year`, `Year-Month`
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_grouped <- group_by(realis,
`Year`, `Year-Month`,
`Planning Region`, `town`, street_name,
Storey_Level, `Flat_Type`)
realis_grouped <- group_by(realis,
`Year`, `Year-Month`,
`Planning Region`, `HDB Town`, street_name,
Storey_Level, `Flat_Type`)
realis_grouped <- group_by(realis,
`Year`, `Year-Month`,
`Planning Region`, `HDB Town`,
Storey_Level, `Flat_Type`)
View(realis_grouped)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Year`, `Year-Month`,
`Planning Region`, `HDB Town`,
Storey_Level)
View(realis_grouped)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Year`, `Year-Month`,
`Planning Region`, `HDB Town`,
Storey_Level)
View(realis_grouped)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap_V5.csv")
realis_grouped <- group_by(realis,
`Year`,
`Planning Region`, `HDB Town`,
Storey_Level)
View(realis_grouped)
packages = c('treemap', 'tidyverse', 'shiny', 'shinydashboard', 'dplyr', 'ggplot2')
for(p in packages){library
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
realis <- read_csv("data/TreeMap.csv")
realis_grouped <- group_by(realis,
`Year`,
`Planning Region`, `HDB Town`,
Storey_Level)
View(realis_grouped)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
Overview <- read_csv("data/Overview1.csv")
Overview <- read_csv("data/Overview1.csv")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
